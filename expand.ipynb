{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_path: str):\n",
    "    \"\"\" A helper functions that loads the file into a list\n",
    "\n",
    "    :param file_path: path to the data file\n",
    "    :return parse data into a list\n",
    "    \"\"\"\n",
    "    data = open(file_path, \"r\").readlines()\n",
    "    return [line.rstrip() for line in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_equations(equations):\n",
    "    \"\"\" A helper functions that parse equations list into a list of tuples.\n",
    "        each tuple contains lhs and rhs of the equation\n",
    "\n",
    "    :param ARRAY of equations\n",
    "    :return factors: (LHS) inputs to the model\n",
    "            expansions: (RHS) group truth\n",
    "    \"\"\"\n",
    "    factors, expansions = zip(*[line.strip().split(\"=\") for line in equations])\n",
    "    return factors, expansions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the train set and parse each line. For each line, separate LHS and RHS of the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "equations = load_file('train.txt')\n",
    "np.random.shuffle(equations)\n",
    "lhses, rhses = parse_equations(equations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using keras Tokenizer, covert the equations to token at character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(equations + ['abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789+-*)('])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eg: equation \"4*a**2+67*a-399\" is converted to token [9, 1, 21, 1, 1, 3, 7, 11, 14, 1, 21, 2, 10, 19, 19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 1, 21, 1, 1, 3, 7, 11, 14, 1, 21, 2, 10, 19, 19]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['4*a**2+67*a-399'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4 * a * * 2 + 6 7 * a - 3 9 9']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[9, 1, 21, 1, 1, 3, 7, 11, 14, 1, 21, 2, 10, 19, 19]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'total number of equations in the dataset 1000001'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"total number of equations in the dataset {}\".format(dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the tokenizer to file to use later during inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_model/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " character to token map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'*': 1,\n",
       " '-': 2,\n",
       " '2': 3,\n",
       " '(': 4,\n",
       " ')': 5,\n",
       " '1': 6,\n",
       " '+': 7,\n",
       " '=': 8,\n",
       " '4': 9,\n",
       " '3': 10,\n",
       " '6': 11,\n",
       " '5': 12,\n",
       " '8': 13,\n",
       " '7': 14,\n",
       " '0': 15,\n",
       " 's': 16,\n",
       " 'n': 17,\n",
       " 'i': 18,\n",
       " '9': 19,\n",
       " 't': 20,\n",
       " 'a': 21,\n",
       " 'c': 22,\n",
       " 'o': 23,\n",
       " 'y': 24,\n",
       " 'z': 25,\n",
       " 'k': 26,\n",
       " 'h': 27,\n",
       " 'j': 28,\n",
       " 'x': 29,\n",
       " 'b': 30,\n",
       " 'd': 31,\n",
       " 'e': 32,\n",
       " 'f': 33,\n",
       " 'g': 34,\n",
       " 'l': 35,\n",
       " 'm': 36,\n",
       " 'p': 37,\n",
       " 'q': 38,\n",
       " 'r': 39,\n",
       " 'u': 40,\n",
       " 'v': 41,\n",
       " 'w': 42}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.EquationSolver"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's divide the dataset into train (95%), validation (2.5%) and test (2.5%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(dataset_size * 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = int(dataset_size *0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(dataset_size *0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size - 950000 \n",
      "validation size - 25000 \n",
      "test size - 25000\n"
     ]
    }
   ],
   "source": [
    "print('train size - {} \\nvalidation size - {} \\ntest size - {}'.format(train_size, valid_size, test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size + valid_size + test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset by considering \"lhs\" as source sequence and \"rhs\" as target sequence. A similar strategy is followed for all the three sets i.e. train, test and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lhs, train_rhs = lhses[0:train_size], rhses[0:train_size]\n",
    "valid_lhs, valid_rhs = lhses[train_size:train_size + valid_size], rhses[train_size:train_size + valid_size]\n",
    "test_lhs, test_rhs = lhses[train_size + valid_size:], rhses[train_size + valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n*(n-13)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lhs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n**2-13*n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rhs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sequence to list of tokens using Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_lhs = tokenizer.texts_to_sequences(train_lhs)\n",
    "encoded_valid_lhs = tokenizer.texts_to_sequences(valid_lhs)\n",
    "encoded_test_lhs = tokenizer.texts_to_sequences(test_lhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_rhs = tokenizer.texts_to_sequences(train_rhs)\n",
    "encoded_valid_rhs = tokenizer.texts_to_sequences(valid_rhs)\n",
    "encoded_test_rhs = tokenizer.texts_to_sequences(test_rhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 9, 1, 16, 1, 4, 6, 13, 2, 12, 1, 16, 5]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test_lhs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 15, 1, 16, 1, 1, 3, 2, 14, 3, 1, 16]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test_rhs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the list of tokens to ragged tensors to make it easy to store and process data with non-uniform shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ragged_tensor(mat):\n",
    "    return tf.ragged.constant(np.array(mat)).to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ragged_tensor(encoded_train_lhs)\n",
    "Y_train = ragged_tensor(encoded_train_rhs)\n",
    "\n",
    "X_valid = ragged_tensor(encoded_valid_lhs)\n",
    "Y_valid = ragged_tensor(encoded_valid_rhs)\n",
    "\n",
    "X_test = ragged_tensor(encoded_test_lhs)\n",
    "Y_test = ragged_tensor(encoded_test_rhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(950000, 29), dtype=int32, numpy=\n",
       "array([[17,  1,  4, ...,  0,  0,  0],\n",
       "       [ 4,  3,  6, ...,  0,  0,  0],\n",
       "       [ 4,  2, 12, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 4,  3,  1, ...,  0,  0,  0],\n",
       "       [ 4, 10,  1, ...,  0,  0,  0],\n",
       "       [ 4,  3,  6, ...,  0,  0,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25000, 29), dtype=int32, numpy=\n",
       "array([[12,  1, 25, ...,  0,  0,  0],\n",
       "       [ 4,  3, 12, ...,  0,  0,  0],\n",
       "       [ 4,  6, 14, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [11,  1, 22, ...,  0,  0,  0],\n",
       "       [ 4,  2,  3, ...,  0,  0,  0],\n",
       "       [ 4,  2, 10, ...,  0,  0,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(950000, 28), dtype=int32, numpy=\n",
       "array([[17,  1,  1, ...,  0,  0,  0],\n",
       "       [ 3,  1, 20, ...,  0,  0,  0],\n",
       "       [ 2, 12,  1, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 6, 15,  1, ...,  0,  0,  0],\n",
       "       [ 6,  3,  1, ...,  0,  0,  0],\n",
       "       [ 2,  6, 13, ...,  0,  0,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X train (950000, 29)\n",
      "shape of Y train (950000, 28)\n",
      "shape of X valid (25000, 29)\n",
      "shape of Y valid (25000, 28)\n",
      "shape of X test (25000, 29)\n",
      "shape of Y test (25000, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of X train {}\".format(X_train.shape))\n",
    "print(\"shape of Y train {}\".format(Y_train.shape))\n",
    "print(\"shape of X valid {}\".format(X_valid.shape))\n",
    "print(\"shape of Y valid {}\".format(Y_valid.shape))\n",
    "print(\"shape of X test {}\".format(X_test.shape))\n",
    "print(\"shape of Y test {}\".format(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = max(tokenizer.word_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numbers from 1 to 42 are attached to one of the character, we can use number 43 indicate start of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = vocab_length + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab_length = vocab_length + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocab_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan is to build a seq2seq model by processing the input sequence (LHS) and predicting the output sequence (LHS). Instead of predicting the output sequence from the final hidden state of the encoder, we feed the output sequence shifted by one to the right. \n",
    "Using this strategy, at each time step decoder gets two inputs 1) encoder output vector and 2) the previous target charatcer\n",
    "\n",
    "So model would require two inputs, input sequence (X_train) and new input (Shifted Output Vector). Let's build the output vector\n",
    "\n",
    "Once we move the output sequence to right, we need to fill the left most empty position with some identifier, and we can use the sos_id (30) for indicating the start of sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_sequence(Y, sos_id = sos_id):\n",
    "    \"\"\" A helper functions to right shift a Tensor and fill the left most place with sos_id\n",
    "\n",
    "    :param Tensor\n",
    "    :return Tensor with sos_id concatenated to the left\n",
    "    \"\"\"\n",
    "    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n",
    "    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_decoder = shift_sequence(Y_train)\n",
    "X_valid_decoder = shift_sequence(Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(950000, 28), dtype=int32, numpy=\n",
       "array([[43, 17,  1, ...,  0,  0,  0],\n",
       "       [43,  3,  1, ...,  0,  0,  0],\n",
       "       [43,  2, 12, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [43,  6, 15, ...,  0,  0,  0],\n",
       "       [43,  6,  3, ...,  0,  0,  0],\n",
       "       [43,  2,  6, ...,  0,  0,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25000, 28), dtype=int32, numpy=\n",
       "array([[43,  2, 12, ...,  0,  0,  0],\n",
       "       [43,  2,  3, ...,  0,  0,  0],\n",
       "       [43,  6, 11, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [43,  6,  3, ...,  0,  0,  0],\n",
       "       [43,  2,  6, ...,  0,  0,  0],\n",
       "       [43,  2, 10, ...,  0,  0,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train_decoder is Y_train with a value 43 (sos_id) prepended to each sequence, which increased the shape of the each sequence from 27 to 28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_output_length = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_output_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each step, the decoder outputs a score for each character in the output vocabulary, and then the softmax layer turns these scores into probabilities\n",
    "\n",
    "attention mechanisms. As their name suggests, these are neural network components that learn to select the part of the inputs that the rest of the model should focus on at each time step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquationSolver(keras.models.Model):\n",
    "    def __init__(self, units=128, encoder_embedding_size=32, decoder_embedding_size=32, input_dim=42, sos_id=43, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # embedding for encoder\n",
    "\n",
    "        self.sos_id = sos_id\n",
    "        self.input_dim = input_dim\n",
    "        \"\"\"\n",
    "        The first layer is an Embedding layer, which will convert character IDs into embeddings.\n",
    "        The embedding matrix needs to have one row per char ID and one column per embedding dimension\n",
    "        Here we are using embedding dimension of 32\n",
    "        Whereas the inputs of the model will be 2D tensors of shape [batch size, time steps] i.e. [None, 31], \n",
    "        the output of the Embedding layer will be a 3D tensor of shape [batch size, time steps, embedding size] ie. [None, 31, 32]\n",
    "        \"\"\"\n",
    "        self.encoder_embedding = keras.layers.Embedding(input_dim= input_dim + 1, output_dim=encoder_embedding_size)\n",
    "\n",
    "        \"\"\"\n",
    "        encoder is LSTM with 128 units\n",
    "        return_state=True when creating the LSTM layer so that we can get its final hidden state and \n",
    "        pass it to the decoder. Note: LSTM cell has two hidden states (short term and long term)\n",
    "        \"\"\"\n",
    "        self.encoder = keras.layers.LSTM(units, return_sequences=True, return_state=True)\n",
    "\n",
    "        \"\"\"\n",
    "        similar to encoder embedding input [batch_size, time steps] o/p [batch_size, time steps, embedding dimension]\n",
    "        \"\"\"\n",
    "        self.decoder_embedding = keras.layers.Embedding(input_dim=input_dim + 2, output_dim=decoder_embedding_size)\n",
    "\n",
    "        \"\"\"\n",
    "        simply wrap the decoder cell in an AttentionWrapper,\n",
    "        we provide the desired attention mechanism, we are using Luong attention for this task\n",
    "        \"\"\"\n",
    "        decoder_cell = keras.layers.LSTMCell(units)\n",
    "        self.luong_attention = tfa.seq2seq.LuongAttention(units)\n",
    "        self.decoder_cell = tfa.seq2seq.AttentionWrapper(cell=decoder_cell, attention_mechanism=self.luong_attention)\n",
    "\n",
    "        # output is Dense Layer with input_dim + 1 units\n",
    "        output_layer = keras.layers.Dense(input_dim + 1)\n",
    "        # decoder while training\n",
    "        self.decoder_training_sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(cell=self.decoder_cell, sampler=self.decoder_training_sampler,\n",
    "                                                output_layer=output_layer,\n",
    "                                                batch_size= BATCH_SIZE)\n",
    "\n",
    "        \"\"\"\n",
    "        decoder while inference, \n",
    "        almost similar to training decoder except we use GreedyEmbeddingSampler and need to provide maximum_iterations.\n",
    "        GreedyEmbeddingSampler: \n",
    "            computes the argmax of the decoder's outputs and the winner is passed through the decoder_embedding\n",
    "            Then it is feed to decoder at the next time step\n",
    "        maximum_iterations: for this task it is set to maximum length of the output sequence in the dataset (29)\n",
    "        \"\"\"\n",
    "        self.decoder_inference_sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(embedding_fn=self.decoder_embedding)\n",
    "        self.inference_decoder = tfa.seq2seq.BasicDecoder(cell=self.decoder_cell,\n",
    "                                                          sampler=self.decoder_inference_sampler,\n",
    "                                                          output_layer=output_layer, maximum_iterations=input_dim)\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        encoder_input, shifted_decoder_inputs = inputs\n",
    "        encoder_embeddings = self.encoder_embedding(encoder_input)\n",
    "        # Note: LSTM cell has two hidden states (short term and long term)\n",
    "        encoder_outputs, encoder_state_h, encoder_state_c = self.encoder(\n",
    "            encoder_embeddings,\n",
    "            training=training)\n",
    "\n",
    "        encoder_state = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "        self.luong_attention(encoder_outputs,\n",
    "                             setup_memory=True)\n",
    "\n",
    "        decoder_embeddings = self.decoder_embedding(shifted_decoder_inputs)\n",
    "\n",
    "        decoder_initial_state = self.decoder_cell.get_initial_state(\n",
    "            decoder_embeddings, batch_size = self.input_dim)\n",
    "        decoder_initial_state = decoder_initial_state.clone(\n",
    "            cell_state=encoder_state)\n",
    "\n",
    "        if training:\n",
    "            decoder_outputs, _, _ = self.decoder(\n",
    "                decoder_embeddings,\n",
    "                initial_state=decoder_initial_state,\n",
    "                training=training\n",
    "                )\n",
    "        else:\n",
    "            start_tokens = tf.zeros_like(encoder_input[:, 0]) + self.sos_id\n",
    "            decoder_outputs, _, _ = self.inference_decoder(\n",
    "                decoder_embeddings,\n",
    "                initial_state=decoder_initial_state,\n",
    "                start_tokens=start_tokens,\n",
    "                end_token=0)\n",
    "\n",
    "        \"\"\"\n",
    "        The decoder outputs (scores) are passed through a softmax\n",
    "        \"\"\"\n",
    "        return tf.nn.softmax(decoder_outputs.rnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"equation_solver\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1376      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  82432     \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      multiple                  1408      \n",
      "_________________________________________________________________\n",
      "LuongAttention (LuongAttenti multiple                  16384     \n",
      "_________________________________________________________________\n",
      "attention_wrapper (Attention multiple                  164352    \n",
      "_________________________________________________________________\n",
      "basic_decoder (BasicDecoder) multiple                  169899    \n",
      "_________________________________________________________________\n",
      "basic_decoder_1 (BasicDecode multiple                  169899    \n",
      "=================================================================\n",
      "Total params: 255,115\n",
      "Trainable params: 255,115\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "we compile this model using the \"sparse_categorical_crossentropy\" loss and Nadam optimizer. Changing optimizer from Adam to Nadam gave a performace boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EquationSolver()\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "29688/29688 [==============================] - 1146s 39ms/step - loss: 0.2803 - accuracy: 0.8973\n",
      "Epoch 2/15\n",
      "29688/29688 [==============================] - 1255s 42ms/step - loss: 0.0823 - accuracy: 0.9679\n",
      "Epoch 3/15\n",
      "29688/29688 [==============================] - 1277s 43ms/step - loss: 0.0556 - accuracy: 0.9787\n",
      "Epoch 4/15\n",
      "29688/29688 [==============================] - 1288s 43ms/step - loss: 0.0435 - accuracy: 0.9835\n",
      "Epoch 5/15\n",
      "29688/29688 [==============================] - 1284s 43ms/step - loss: 0.0359 - accuracy: 0.9863\n",
      "Epoch 6/15\n",
      "29688/29688 [==============================] - 1329s 45ms/step - loss: 0.0334 - accuracy: 0.9874\n",
      "Epoch 7/15\n",
      "29688/29688 [==============================] - 1360s 46ms/step - loss: 0.0284 - accuracy: 0.9892\n",
      "Epoch 8/15\n",
      "29688/29688 [==============================] - 1300s 44ms/step - loss: 0.0259 - accuracy: 0.9903\n",
      "Epoch 9/15\n",
      "29688/29688 [==============================] - 1299s 44ms/step - loss: 0.0237 - accuracy: 0.9911\n",
      "Epoch 10/15\n",
      "29688/29688 [==============================] - 1338s 45ms/step - loss: 0.0218 - accuracy: 0.9918\n",
      "Epoch 11/15\n",
      "29688/29688 [==============================] - 2244s 76ms/step - loss: 0.0205 - accuracy: 0.9923\n",
      "Epoch 12/15\n",
      "29688/29688 [==============================] - 1305s 44ms/step - loss: 0.0195 - accuracy: 0.9927\n",
      "Epoch 13/15\n",
      "29688/29688 [==============================] - 1395s 47ms/step - loss: 0.0188 - accuracy: 0.9931\n",
      "Epoch 14/15\n",
      "29688/29688 [==============================] - 1412s 48ms/step - loss: 0.0178 - accuracy: 0.9934\n",
      "Epoch 15/15\n",
      "29688/29688 [==============================] - 1352s 46ms/step - loss: 0.0168 - accuracy: 0.9938\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_weights to tuned weights to use later during inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss & history')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\n",
    "    'saved_model/equ_model_2', overwrite=True, save_format=None, options=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('saved_model/equ_model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_new_sequences(factors, max_input_length = 29, max_output_length = 28):\n",
    "    seqs = tokenizer.texts_to_sequences(factors)\n",
    "    X_new = ragged_tensor(seqs)\n",
    "\n",
    "    if X_new.shape[1] < max_input_length:\n",
    "        X_new = tf.pad(X_new, [[0, 0], [0, max_input_length - X_new.shape[1]]])\n",
    "    X_decoder = tf.zeros(shape=(len(X_new), max_output_length), dtype=tf.int32)\n",
    "    return X_new, X_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seqs(factors):\n",
    "    X_new, X_decoder = prepare_new_sequences(factors)\n",
    "    Y_probas = model.predict([X_new, X_decoder])\n",
    "\n",
    "    Y_pred = tf.argmax(Y_probas, axis=-1)\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [test_lhs, test_rhs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'w') as f:\n",
    "    for l, r in zip(test_lhs, test_rhs):\n",
    "        f.write(\"%s=%s\\n\" % (l, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for e in chunks(test_set[0], 32):\n",
    "    ids = predict_seqs(e)\n",
    "    results.append(tokenizer.sequences_to_texts(ids.numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_result(r):\n",
    "    return r.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(true_expansion: str, pred_expansion: str) -> int:\n",
    "    \"\"\" the scoring function - this is how the model will be evaluated\n",
    "\n",
    "    :param true_expansion: group truth string\n",
    "    :param pred_expansion: predicted string\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return int(true_expansion == pred_expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "results = list(itertools.chain(*results))\n",
    "results = list(map(process_result, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = test_set[1]\n",
    "count = len(expected)\n",
    "correct_count = 0\n",
    "\n",
    "for a, b in zip(results, expected):\n",
    "    correct_count += score(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'total score on test set 0.8388'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"total score on test set {}\".format(correct_count/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "references:\n",
    "\n",
    "https://www.tensorflow.org/addons/api_docs/python/tfa/seq2seq/BasicDecoder\n",
    "\n",
    "https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt\n",
    "\n",
    "https://www.tensorflow.org/addons/api_docs/python/tfa/seq2seq/LuongAttention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
